{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# MCIS6273 Data Mining (Prof. Maull) / Fall 2021 / HW4 [BONUS]\n", "\n", "**This assignment is worth up to 20 POINTS to your grade total if you complete it on time.**\n", "\n", "| Points <br/>Possible | Due Date | Time Commitment <br/>(estimated) |\n", "|:---------------:|:--------:|:---------------:|\n", "| 20 | Wednesday, December 7  @ Midnight | _up to_ 8 hours |\n", "\n", "\n", "* **GRADING:** Grading will be aligned with the completeness of the objectives.\n", "\n", "* **INDEPENDENT WORK:** Copying, cheating, plagiarism  and academic dishonesty _are not tolerated_ by University or course policy.  Please see the syllabus for the full departmental and University statement on the academic code of honor.\n", "\n", "## OBJECTIVES\n", "* Continue to improve your Bayesian solution from HW3b, all earned points are bonus/extra credit\n", "\n", "## WHAT TO TURN IN\n", "You are being encouraged to turn the assignment in using the provided\n", "Jupyter Notebook.  To do so, make a directory in your Lab environment called\n", "`homework/hwN`.   Put all of your files in that directory.  Then zip that directory,\n", "rename it with your name as the first part of the filename (e.g. `maull_hwN_files.zip`), then\n", "download it to your local machine, then upload the `.zip` to Blackboard.\n", "\n", "If you do not know how to do this, please ask, or visit one of the many tutorials out there\n", "on the basics of using zip in Linux.\n", "\n", "If you choose not to use the provided notebook, you will still need to turn in a\n", "`.ipynb` Jupyter Notebook and corresponding files according to the instructions in\n", "this homework.\n", "\n", "\n", "## ASSIGNMENT TASKS\n", "### (100%) Continue to improve your Bayesian solution from HW3b, all earned points are bonus/extra credit \n", "\n", "In HW3b you learned some of the basics of document classification \n", "through the Multinomial N&iuml;ave Bayes  model within SciKit-Learn.\n", "\n", "This is a BONUS homework, so anything you do will get you partial or\n", "full points.  If you complete all of the parts you will\n", "earn up to 20 points depending on the completeness of your solution.\n", "\n", "There are four ways to get points:\n", "\n", "1. build a better training set,\n", "2. train the model on new training sets (either one's you found from (1) or \n", "   some other corpus),\n", "3. improve the model analysis by detailing the summary of model accuracy across\n", "   all topics,\n", "4. develop a visualization of the data, model output or some other \n", "   interesting component of HW3b.\n", "\n", "Doing all three completely will earn up to 20 points and a combination of\n", "all three will earn a few extra points depending on completeness.\n", "\n", "&#167;  We learned that the size of the training corpus matters.\n", "\n", "To earn up to 5 points, you will make a new dataset for the other\n", "disciplines (1 point for each one of sociology, econonics, education,\n", "physics and computer science, 5 full points if you do them all).\n", "\n", "You solutions may vary, but here are a few ideas:\n", "\n", "* find the top 100 to 200 papers in a discipline, get their DOIs and then \n", "  get the abstracts from Semantic Scholar.  Use those abstracts to build\n", "  a new dataset;\n", "* find the top 20 papers in a discipline and extract the text from that \n", "  20 then all the subsequent papers that that paper cited.  For example,\n", "  if you look at the paper which won the 2010 Nobel Prize in Physics and\n", "  then look at all the papers which that paper cited (you can look at\n", "  the output of semantic scholar), then get the abstracts of those papers,\n", "  you would end up with a large diverse corpus (though it may be a bit\n", "  narrow depending on the papers, hence why you would need a good \n", "  sample of the 20 seed papers);\n", "* pull tens (minimally) of books from [Gutenberg.org](https://gutenberg.org) in the disciplines\n", "  of interest and train on those documents;\n", "* pull fulltext from the PDF (or other sources)  of classic / seminal\n", "  papers in a discipline, convert them to text and then use them as \n", "  the training documents in your corpus for the discipline.\n", "\n", "\n", "Varying levels of completeness accompanying good explanations of what you're \n", "doing and _why_ will earn you more points towards the 5.\n", "\n", "\n", "&#167;  Train the new model on the dataset (or datasets) from the first\n", "task and then analyze the performance of the model.\n", "\n", "This analysis would include the increased accuracy, with \n", "an explanation why you feel the accuracy has increased\n", "and if it doesn't change, why you think it did not.\n", "\n", "Point increases can range up to 5, and the completeness\n", "of your analysis will earn you more points.\n", "\n", "An evaluation of true positive and false positive rates\n", "would be valuable to include in your analysis.  Any \n", "error analysis would be important anyway no matter\n", "if you are analyzing the performance of one class\n", "or all of them.\n", "\n", "\n", "&#167;  Visualize the data of the corpus in some interesting way,\n", "hopefully with the aim of improving the understanding of the\n", "underlying data.\n", "\n", "Here are a few ideas:\n", "\n", "* use Python to develop a word cloud of the corpus _without_\n", "  stop words and/or other irrelevant words;\n", "* compare each discipline's most relevant words through the\n", "  lens of the TFIDFVectorizer which will require you to \n", "  tap into the underlying representation of that object\n", "  and doing something useful with the data that is there;\n", "* show how a corpus changes after expanding the underlying\n", "  abstracts (e.g. show the dominant words in the economics\n", "  corpus after 100 documents, then how it changes after\n", "  200 documents);\n", "* do a time series visualization of a sub-discipline.  For\n", "  example, looking at computer science from 1995 - 2015\n", "  in 5 year increments and taking the top documents\n", "  and their differences. What you might relay in such\n", "  a visualization is how topics change over 20 years time.\n", "\n", "There are many other things you might think of.  Look\n", "at the visualizations in the various Python libraries\n", "([Seaborn](https://seaborn.pydata.org), [matplotlib](https://matplotlib.org) \n", "and others) to get some inspiration.\n", "\n", "You can earn up to 5 points for this part.\n", "\n", "\n", "\n"]}], "metadata": {"anaconda-cloud": {}, "kernelspec": {"display_name": "Python [default]", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.1"}, "toc": {"colors": {"hover_highlight": "#DAA520", "navigate_num": "#000000", "navigate_text": "#333333", "running_highlight": "#FF0000", "selected_highlight": "#FFD700", "sidebar_border": "#EEEEEE", "wrapper_background": "#FFFFFF"}, "moveMenuLeft": true, "nav_menu": {"height": "12px", "width": "252px"}, "navigate_menu": true, "number_sections": false, "sideBar": true, "threshold": "1", "toc_cell": false, "toc_section_display": "block", "toc_window_display": true, "widenNotebook": false}}, "nbformat": 4, "nbformat_minor": 0}